# -*- coding: utf-8 -*-
"""trainUnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mLIgKOUCJIP0qVK98nw4SFLzRkTsoTWC
"""

from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras import backend as keras

# !pip install SimpleITK
import SimpleITK as sitk
import os
import numpy as np
import matplotlib.pyplot as plt

# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools
# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
# !apt-get update -qq 2>&1 > /dev/null
# !apt-get -y install -qq google-drive-ocamlfuse fuse
# from google.colab import auth
# auth.authenticate_user()
# from oauth2client.client import GoogleCredentials
# creds = GoogleCredentials.get_application_default()
# import getpass
# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
# vcode = getpass.getpass()
# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

# !mkdir -p drive
# !google-drive-ocamlfuse drive

def unet(pretrained_weights = None,input_size = (96,96,1)):
    inputs = Input(input_size)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)

    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)

    model = Model(input = inputs, output = conv10)

    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])
    
    model.summary()

    if(pretrained_weights):
    	model.load_weights(pretrained_weights)

    return model

def find_images(path_dir):
    num=10
    X_train=np.zeros((274*num,96,96),dtype=np.uint16)
    Y_train=np.zeros((274*num,96,96),dtype=np.uint16)
    j=0
    for item in os.listdir(path_dir):
        item=os.path.join(path_dir,item)
        for item2 in os.listdir(item):
            im ={'T1':None,'gt':None}
            item2=os.path.join(item,item2)
            for item3 in os.listdir(item2):
                item3=os.path.join(item2,item3)
                for item4 in os.listdir(item3):
                    item5=os.path.join(item3,item4)
                    if os.path.isfile(item5) and item5.endswith('.mha'):
                        itk_image = sitk.ReadImage(item5)
                        nd_image = sitk.GetArrayFromImage(itk_image)
                        if 'more' in item5 or 'OT' in item5:
                            im['gt']=nd_image
                        elif 'T1' in item5:
                            im['T1']=nd_image
            #print(item2)
            for i in range(70,70+num):
                Y_train[j]=np.where(im['gt'][i,24:216:2,24:216:2] > 0, 1, 0)
                X_train[j]=im['T1'][i,24:216:2,24:216:2]
                j+=1

    return X_train,Y_train

# path_dir="drive/BRATS2015_Training"

# X_train,Y_train=find_images(path_dir)
# X1_train=X_train/np.amax(X_train)
# #Y1_train=Y_train-np.mean(Y_train,axis=0)
# shape=X1_train.shape
# X1_train=X1_train.reshape(shape[0],shape[1],shape[2],1)
# Y_train=Y_train.reshape(shape[0],shape[1],shape[2],1)

with h5py.File('./x_train.h5', 'r') as hf:
    X1_train = hf['x_train'][:]
with h5py.File('./y_train.h5', 'r') as hf:
    Y_train = hf['y_train'][:]

model = unet(input_size = (96,96,1))

model_checkpoint = ModelCheckpoint('./unet_brat3.hdf5', monitor='val_loss',verbose=1, save_best_only=True)
model.fit(X1_train, Y_train, batch_size=32, nb_epoch=100, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint],class_weight=[1,26])


model.save('unet_train.hdf5')
# !nvidia-smi
# import tensorflow as tf
# print(tf.test.is_built_with_cuda())

# 1/np.mean(Y_train)

# from PIL import Image

# img = Image.open("/media/pushpendra/DATA/4th sem/AML/assignment1/normalsVsAbnormalsV1/normalsJPG/IMG-0001-00001.jpg").convert('L')

# img= img.resize((113,113),Image.ANTIALIAS)

# a = np.asarray(img)
# print(a.shape)
# #a.reshape(240,240,1)
# # plt.imshow(a)
# plt.imshow(a[13:213:2,13:213:2])

# def find_test_images(path_dir):
#     X_test=np.zeros((942,96,96),dtype=np.uint16)
#     Y_test=np.zeros((942,2),dtype=np.uint16)
#     j=0
#     k=1
#     for item in os.listdir(path_dir):
#         item=os.path.join(path_dir,item)
#         for item2 in os.listdir(item):
#             item5=os.path.join(item,item2)
#             if os.path.isfile(item5) and item5.endswith('.jpg'):
#                 image = Image.open(item5).convert('L')
#                 image = image.resize((96,96),Image.ANTIALIAS)
#                 X_test[j]=image
#                 Y_test[j][k]=1
#                 j+=1
#         k-=1
#     return X_test,Y_test

# test_path="/media/pushpendra/DATA/4th sem/AML/assignment1/normalsVsAbnormalsV1"
# X_test,Y_test = find_test_images(test_path)

# X_train=X_train-mean

# np.save("mean",np.mean(X_test,axis=0))

# mean = np.load("mean.npy")

# shape=X_train.shape
# X_train=X_train.reshape(shape[0],shape[1],shape[2],1)

# X_test=X1_train
# Y_test=Y_train

# #model = unet()
# # model.load_weights("drive/unet_brat.hdf5")
# results = model.predict(X_test[0:100,:,:,:],30,verbose=1)

# results.shape

# plt.imshow(results[60].reshape(96,96),cmap='gray')

# plt.imshow(X1_train[60].reshape(96,96))

# X1.shape

# plt.imshow(Y_train[60].reshape(96,96),cmap='gray')

# plt.imshow(X_test[6]-np.mean(X_test,axis=0))

# plt.imshow(np.mean(X_test,axis=0))

# plt.imshow(Y_test[10].reshape(240,240))

# plt.imshow(np.where(Y_test[10] > 0, 1, 0).reshape(240,240), cmap='gray')

# np.where(Y_test[10] > 0, 1, 0)

# results[6].reshape(240,240)

# import nibabel as nib
# img = nib.load("../nii/002_brain.nii.gz")

# data=img.get_fdata()

# data.shape

# plt.imshow(data[50:210,140,:])

# from skimage.transform import resize

# # image = color.rgb2gray(data.astronaut())

# # image_rescaled = rescale(image, 1.0 / 4.0, anti_aliasing=False)
# image_resized = resize(data[50:210,140,:], (240, 240))

# plt.imshow(image_resized)

# image_resized.shape

# epoch_frames = 0
# def visualize(im, actual_target, predicted_target):
#     im = im[0,:,:,0]
#     actual_target = actual_target[0,:,:,0]
#     predicted_target = predicted_target[0,:,:,0]
    
#     plt.figure(1,(10,4))
#     plt.subplot(1, 4, 1)
#     plt.title("Image")
#     plt.imshow(im, cmap = "gray") # orientation='horizontal',fraction=0.046, pad=0.04
# #     plt.axis('off')
#     plt.subplot(1, 4, 2)
#     plt.title("Ground Truth")
#     plt.imshow(actual_target, cmap = "magma",vmin=0, vmax=1)
#     plt.axis('off')
#     plt.subplot(1, 4, 3)
#     plt.title("Prediction")
#     plt.imshow(predicted_target, cmap = "magma",vmin=0, vmax=1)
#     plt.axis('off')
#     plt.subplot(1, 4, 4)
#     plt.axis('off')
#     t = 'Class: ' + str(int(np.sum(predicted_target.ravel()) > 10))
#     plt.text(0.5, 0.5, t, ha = 'center',va='top',wrap=True)
#     global epoch_frames
#     epoch_frames += 1
#     plt.suptitle('epoch: ' + str(epoch_frames))
#     plt.savefig("epoch_frames/frame"+str(epoch_frames)+ '.png')
# #     plt.show(block=False)
#     plt.close()

# # Custom callback

# visualize()